{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score \n",
    "bmac = balanced_accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "#%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_file = 'X_train.csv'\n",
    "y_train_file = 'y_train.csv'\n",
    "X_test_file  = 'X_test.csv'\n",
    "\n",
    "labels_df = pd.read_csv(y_train_file)\n",
    "bare_df   = pd.read_csv(X_train_file)\n",
    "test_df   = pd.read_csv(X_test_file)\n",
    "\n",
    "bare_df.insert(0,'y',labels_df['y'])\n",
    "\n",
    "labels_df = labels_df.set_index('id')\n",
    "bare_df   = bare_df  .set_index('id')\n",
    "test_df   = test_df  .set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('bare_df.shape =', bare_df.shape)\n",
    "print('test_df.shape  =', test_df.shape)\n",
    "print('y distribution: ')\n",
    "print(bare_df['y'].value_counts())\n",
    "print('We are told that \\\"Test set has the same class imbalance as the training set.\\\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_std_normalise(df):\n",
    "    return (df-df.mean())/df.std()\n",
    "\n",
    "def min_max_normalise(df):\n",
    "    return (2*df-df.min()-df.max())/(df.max()-df.min())\n",
    "\n",
    "def oh(y):\n",
    "    y0 = y == 0\n",
    "    y1 = y == 1\n",
    "    y2 = y == 2\n",
    "    return np.stack((y0,y1,y2),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_oversampling = False\n",
    "def preprocess(split):\n",
    "    \n",
    "    #Normalizing and preparing feature and label matrices\n",
    "    nrm_df = mean_std_normalise(bare_df.drop(columns='y'))\n",
    "    nrm_df['y'] = bare_df['y']\n",
    "    # dividing X, y into train and test data \n",
    "    df_train, df_test = train_test_split(nrm_df, test_size = split)\n",
    "    if use_oversampling:\n",
    "    #Oversampling the training set only\n",
    "        # Class count\n",
    "        count_1, count_2, count_0 = df_train['y'].value_counts()\n",
    "\n",
    "        # Divide by class\n",
    "        df_0 = df_train[df_train['y'] == 0]\n",
    "        df_1 = df_train[df_train['y'] == 1]\n",
    "        df_2 = df_train[df_train['y'] == 2]\n",
    "        # Let's try balancing out the classes with over-sampling\n",
    "        df_0_over = df_0.sample(count_1, replace=True)\n",
    "        df_2_over = df_2.sample(count_1, replace=True)\n",
    "        over_df = pd.concat([df_0_over, df_1, df_2_over], axis=0)\n",
    "        # I expect this to be bigger, count1*3 = 10800\n",
    "        over_df.shape\n",
    "\n",
    "        X_train = over_df.drop(columns='y').values\n",
    "        y_train = over_df['y'].values\n",
    "        weights = {0:1, 1:1, 2:1}\n",
    "    else:\n",
    "        X_train = df_train.drop(columns='y').values\n",
    "        y_train = df_train['y'].values\n",
    "        y_AML   = test_df.values\n",
    "        weights={0:1/.125, 1:1/.75, 2:1/.125}\n",
    "\n",
    "    n_train = len(y_train)\n",
    "\n",
    "    X_test = df_test.drop(columns='y').values\n",
    "    y_test = df_test['y'].values\n",
    "    n_test = len(y_test)\n",
    "    print(\"train shape = \", np.shape(X_train))\n",
    "    print(\"test shape  = \", np.shape(X_test))\n",
    "    print()\n",
    "    print('n0test = %.1f%%'%(100/n_test*np.sum((y_test == 0))))\n",
    "    print('n1test = %.1f%%'%(100/n_test*np.sum((y_test == 1))))\n",
    "    print('n2test = %.1f%%'%(100/n_test*np.sum((y_test == 2))))\n",
    "    print()\n",
    "    print('n0train = %.1f%%'%(100/n_train*np.sum((y_train == 0))))\n",
    "    print('n1train = %.1f%%'%(100/n_train*np.sum((y_train == 1))))\n",
    "    print('n2train = %.1f%%'%(100/n_train*np.sum((y_train == 2))))\n",
    "    print()\n",
    "    return X_train, y_train, weights, X_test, y_test, n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    " #Initialize with whatever parameters you want to\n",
    "\n",
    "rfc100 = RandomForestClassifier(n_jobs=-1, \n",
    "                                class_weight=weights,\n",
    "                                n_estimators=20, \n",
    "                                min_samples_split=10, \n",
    "                                min_samples_leaf=10,\n",
    "                                max_features = 'auto')\n",
    "# 10-Fold Cross validation\n",
    "#print(np.mean(cross_val_score(rfc100, X_train, y_train, cv=10)))\n",
    "rfc100.fit(X_train, y_train)\n",
    "print('Fitted')\n",
    "rfc100_pred  = rfc100.predict(X_test)\n",
    "print('Plain predictions')\n",
    "print('test  BMAC =', bmac(y_test, rfc100_pred))\n",
    "rfc100_train = rfc100.predict(X_train)\n",
    "print('train BMAC =', bmac(y_train, rfc100_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "abc100 = AdaBoostClassifier(n_estimators=100)\n",
    "abc100.fit(X_train, y_train)  \n",
    "\n",
    "print('Fitted')\n",
    "abc100_pred  = abc100.predict(X_test)\n",
    "print('Plain predictions')\n",
    "print('test  BMAC =', bmac(y_test, abc100_pred))\n",
    "abc100_train = abc100.predict(X_train)\n",
    "print('train BMAC =', bmac(y_train, abc100_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "abc100.feature_importances_  \n",
    "\n",
    "abc100.score(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC \n",
    "rbf1 = SVC(gamma='scale', class_weight=weights, kernel = 'rbf', C = 1)\n",
    "rbf1 = rbf1.fit(X_train, y_train)\n",
    "print('Fitted')\n",
    "rbf1_pred  = rbf1.predict(X_test)\n",
    "print('Plain predictions')\n",
    "print('test  BMAC =', bmac(y_test, rbf1_pred))\n",
    "rbf1_train = rbf1.predict(X_train)\n",
    "print('train BMAC =', bmac(y_train, rbf1_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Argmax predictions')\n",
    "rbf1_pred_prob  = rbf1.decision_function(X_test)\n",
    "print('test  BMAC =', bmac(y_test, np.argmax(rbf1_pred_prob, axis=1)))\n",
    "rbf1_train_prob = rbf1.decision_function(X_train)\n",
    "print('train BMAC =', bmac(y_train, np.argmax(rbf1_train_prob, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples it can't confidently classify are in the middle\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "ax1 = plt.figure().add_subplot(111, projection='3d')\n",
    "ax2 = plt.figure().add_subplot(111, projection='3d')\n",
    "ax1.scatter(rbf1_train_prob[:,0], rbf1_train_prob[:,1], rbf1_train_prob[:,2], c=y_train,s=1)\n",
    "ax2.scatter(rbf1_pred_prob[:,0], rbf1_pred_prob[:,1], rbf1_pred_prob[:,2], c=y_test, s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf100 = SVC(gamma='scale',class_weight=weights, kernel = 'rbf', \n",
    "          C = 1.00, decision_function_shape='ovo')\n",
    "rbf100 = rbf.fit(X_train, y_train)\n",
    "print('Fitted')\n",
    "rbf100_pred  = rbf.predict(X_test)\n",
    "print('test  BMAC =', bmac(y_test, rbf100_pred))\n",
    "rbf100_train = rbf.predict(X_train)\n",
    "print('train BMAC =', bmac(y_train, rbf100_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = {0:1/.125, 1:1/.75, 2:1/.125}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([*w.values()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A host of Scikit-learn models\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "def get_models():\n",
    "    \"\"\"Generate a library of base learners.\"\"\"\n",
    "    nb = GaussianNB()\n",
    "    svc = SVC(C=100, probability=True)\n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    lr = LogisticRegression(C=100, random_state=SEED)\n",
    "    nn = MLPClassifier((80, 10), early_stopping=False, random_state=SEED)\n",
    "    gb = GradientBoostingClassifier(n_estimators=100, random_state=SEED)\n",
    "    rf = RandomForestClassifier(n_estimators=10, max_features=3, random_state=SEED)\n",
    "\n",
    "    models = {'svm': svc,\n",
    "              'knn': knn,\n",
    "              'naive bayes': nb,\n",
    "              'mlp-nn': nn,\n",
    "              'random forest': rf,\n",
    "              'gbm': gb,\n",
    "              'logistic': lr,\n",
    "              }\n",
    "\n",
    "    return models\n",
    "\n",
    "\n",
    "def train_predict(model_list):\n",
    "    \"\"\"Fit models in list on training set and return preds\"\"\"\n",
    "    P = np.zeros((ytest.shape[0], len(model_list)))\n",
    "    P = pd.DataFrame(P)\n",
    "\n",
    "    print(\"Fitting models.\")\n",
    "    cols = list()\n",
    "    for i, (name, m) in enumerate(models.items()):\n",
    "        print(\"%s...\" % name, end=\" \", flush=False)\n",
    "        m.fit(xtrain, ytrain)\n",
    "        P.iloc[:, i] = m.predict_proba(xtest)[:, 1]\n",
    "        cols.append(name)\n",
    "        print(\"done\")\n",
    "\n",
    "    P.columns = cols\n",
    "    print(\"Done.\\n\")\n",
    "    return P\n",
    "\n",
    "\n",
    "def score_models(P, y):\n",
    "    \"\"\"Score model in prediction DF\"\"\"\n",
    "    print(\"Scoring models.\")\n",
    "    for m in P.columns:\n",
    "        score = roc_auc_score(y, P.loc[:, m])\n",
    "        print(\"%-26s: %.3f\" % (m, score))\n",
    "    print(\"Done.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Loading the Digits dataset\n",
    "#digits = datasets.load_digits()\n",
    "\n",
    "# To apply an classifier on this data, we need to flatten the image, to\n",
    "# turn the data in a (samples, feature) matrix:\n",
    "n_samples = len(y_train)+len(y_test)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'gamma': [3e-3, 1e-4, 3e-4],\n",
    "                     'C': [1, 1.001]}]\n",
    "\n",
    "clf = GridSearchCV(SVC(kernel='rbf', class_weight=weights),tuned_parameters, cv=5,\n",
    "                   scoring='balanced_accuracy',\n",
    "                   n_jobs=-1, verbose=15)\n",
    "\n",
    "#tuned_parameters = [n_estimators=100,  class_weight=weights)\n",
    "\n",
    "#clf = GridSearchCV(RandomForestClassifier(),tuned_parameters, cv=5,\n",
    "#                   scoring='balanced_accuracy',\n",
    "#                   n_jobs=-1, verbose=10)\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(clf.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "print()\n",
    "\n",
    "clf_pred  = clf.predict(X_test)\n",
    "print('Plain predictions')\n",
    "print('test  BMAC =', bmac(y_test, clf_pred))\n",
    "clf_train = clf.predict(X_train)\n",
    "print('train BMAC =', bmac(y_train, clf_train))\n",
    "\n",
    "print(\"Detailed classification report:\")\n",
    "print()\n",
    "print(\"The model is trained on the full development set.\")\n",
    "print(\"The scores are computed on the full evaluation set.\")\n",
    "print()\n",
    "print(classification_report(y_test, clf_pred))\n",
    "print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Best parameters set found on development set:\n",
    "\n",
    "{'C': 1.0, 'gamma': 0.0001}\n",
    "\n",
    "Grid scores on development set:\n",
    "\n",
    "0.538 (+/-0.053) for {'C': 1.0, 'gamma': 0.003}\n",
    "0.686 (+/-0.027) for {'C': 1.0, 'gamma': 0.0001}\n",
    "0.671 (+/-0.035) for {'C': 1.0, 'gamma': 0.0003}\n",
    "0.535 (+/-0.051) for {'C': 1.2, 'gamma': 0.003}\n",
    "0.680 (+/-0.027) for {'C': 1.2, 'gamma': 0.0001}\n",
    "0.670 (+/-0.042) for {'C': 1.2, 'gamma': 0.0003}\n",
    "0.532 (+/-0.054) for {'C': 1.5, 'gamma': 0.003}\n",
    "0.683 (+/-0.020) for {'C': 1.5, 'gamma': 0.0001}\n",
    "0.665 (+/-0.037) for {'C': 1.5, 'gamma': 0.0003}\n",
    "\n",
    "Plain predictions\n",
    "test  BMAC = 0.7203832163671522\n",
    "train BMAC = 0.7913527042800608\n",
    "Detailed classification report:\n",
    "\n",
    "The model is trained on the full development set.\n",
    "The scores are computed on the full evaluation set.\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.37      0.63      0.46       166\n",
    "           1       0.95      0.71      0.81       891\n",
    "           2       0.48      0.82      0.60       143\n",
    "\n",
    "    accuracy                           0.71      1200\n",
    "   macro avg       0.60      0.72      0.63      1200\n",
    "weighted avg       0.81      0.71      0.74      1200"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Best parameters set found on development set:\n",
    "\n",
    "{'C': 1.5, 'gamma': 0.0001}\n",
    "\n",
    "Grid scores on development set:\n",
    "\n",
    "0.625 (+/-0.045) for {'C': 1.5, 'gamma': 0.001}\n",
    "0.683 (+/-0.020) for {'C': 1.5, 'gamma': 0.0001}\n",
    "0.657 (+/-0.051) for {'C': 1.5, 'gamma': 1e-05}\n",
    "0.618 (+/-0.056) for {'C': 2.5, 'gamma': 0.001}\n",
    "0.677 (+/-0.047) for {'C': 2.5, 'gamma': 0.0001}\n",
    "0.669 (+/-0.047) for {'C': 2.5, 'gamma': 1e-05}\n",
    "0.617 (+/-0.064) for {'C': 3, 'gamma': 0.001}\n",
    "0.673 (+/-0.043) for {'C': 3, 'gamma': 0.0001}\n",
    "0.671 (+/-0.052) for {'C': 3, 'gamma': 1e-05}\n",
    "0.612 (+/-0.071) for {'C': 3.5, 'gamma': 0.001}\n",
    "0.669 (+/-0.046) for {'C': 3.5, 'gamma': 0.0001}\n",
    "0.677 (+/-0.044) for {'C': 3.5, 'gamma': 1e-05}\n",
    "0.611 (+/-0.070) for {'C': 4, 'gamma': 0.001}\n",
    "0.669 (+/-0.049) for {'C': 4, 'gamma': 0.0001}\n",
    "0.678 (+/-0.039) for {'C': 4, 'gamma': 1e-05}\n",
    "\n",
    "Plain predictions\n",
    "test  BMAC = 0.7159226563510366\n",
    "train BMAC = 0.8207156596206812\n",
    "Detailed classification report:\n",
    "\n",
    "The model is trained on the full development set.\n",
    "The scores are computed on the full evaluation set.\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.37      0.63      0.47       166\n",
    "           1       0.95      0.73      0.82       891\n",
    "           2       0.47      0.79      0.59       143\n",
    "\n",
    "    accuracy                           0.72      1200\n",
    "   macro avg       0.60      0.72      0.63      1200\n",
    "weighted avg       0.81      0.72      0.75      1200"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Best parameters set found on development set:\n",
    "\n",
    "{'C': 3, 'gamma': 0.0001}\n",
    "\n",
    "Grid scores on development set:\n",
    "\n",
    "0.406 (+/-0.025) for {'C': 1, 'gamma': 0.01}\n",
    "0.670 (+/-0.035) for {'C': 1, 'gamma': 0.001}\n",
    "0.697 (+/-0.037) for {'C': 1, 'gamma': 0.0001}\n",
    "0.404 (+/-0.025) for {'C': 3, 'gamma': 0.01}\n",
    "0.637 (+/-0.051) for {'C': 3, 'gamma': 0.001}\n",
    "0.699 (+/-0.027) for {'C': 3, 'gamma': 0.0001}\n",
    "0.404 (+/-0.025) for {'C': 10, 'gamma': 0.01}\n",
    "0.622 (+/-0.054) for {'C': 10, 'gamma': 0.001}\n",
    "0.679 (+/-0.030) for {'C': 10, 'gamma': 0.0001}\n",
    "0.404 (+/-0.025) for {'C': 30, 'gamma': 0.01}\n",
    "0.623 (+/-0.049) for {'C': 30, 'gamma': 0.001}\n",
    "0.641 (+/-0.020) for {'C': 30, 'gamma': 0.0001}\n",
    "\n",
    "Plain predictions\n",
    "test  BMAC = 0.6889743306689894\n",
    "train BMAC = 0.8782967516049084\n",
    "Detailed classification report:\n",
    "\n",
    "The model is trained on the full development set.\n",
    "The scores are computed on the full evaluation set.\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.33      0.62      0.43       143\n",
    "           1       0.96      0.75      0.84       919\n",
    "           2       0.44      0.70      0.54       138\n",
    "\n",
    "    accuracy                           0.73      1200\n",
    "   macro avg       0.58      0.69      0.60      1200\n",
    "weighted avg       0.83      0.73      0.76      1200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, weights, X_test, y_test, n_test = preprocess(0.01)\n",
    "samwe = [weights[int(yi)] for yi in y_test] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='rbf', \n",
    "          class_weight=weights, \n",
    "          gamma=0.8e-4, C=1, # result of cross validation\n",
    "          probability=True)\n",
    "\n",
    "svc.fit(X_train, y_train)\n",
    "svc_pred  = svc.predict(X_test)\n",
    "svc_train = svc.predict(X_train)\n",
    "print('Plain predictions')\n",
    "print('train BMAC =', bmac(y_train, svc_train))\n",
    "print('test  BMAC =', bmac(y_test, svc_pred))\n",
    "print(classification_report(y_test, svc_pred, sample_weight = samwe))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_pr_conf = svc.predict_proba(X_test)\n",
    "pr_confidence = [svc_pr_conf[i,y_test[i]] for i in range(n_test)]\n",
    "plt.hist(pr_confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_tr_conf = svc.predict_proba(X_train)\n",
    "tr_confidence = [svc_tr_conf[i,y_train[i]] for i in range(4752)]\n",
    "plt.hist(tr_confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from matplotlib import pyplot as plt\n",
    "plt.hist((pr_confidence, tr_confidence, AML_confidence))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_AML = mean_std_normalise(test_df).values\n",
    "np.shape(X_AML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML_y  = svc.predict(X_AML)\n",
    "AML_conf  = svc.predict_proba(X_AML)\n",
    "AML_confidence = [AML_conf[i,AML_y[i]] for i in range(4100)]\n",
    "plt.hist(AML_confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(AML_confidence)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = len(y_train)+len(y_test)\n",
    "\n",
    "tuned_parameters = [{'gamma': [0.8e-4],\n",
    "                     'C': [1]}]\n",
    "\n",
    "clf = GridSearchCV(SVC(kernel = 'rbf', class_weight = weights, probability=True),\n",
    "                   tuned_parameters, \n",
    "                   cv = 5,\n",
    "                   scoring='balanced_accuracy',\n",
    "                   n_jobs=-1, verbose=15)\n",
    "\n",
    "#tuned_parameters = [n_estimators=100,  class_weight=weights)\n",
    "\n",
    "#clf = GridSearchCV(RandomForestClassifier(),tuned_parameters, cv=5,\n",
    "#                   scoring='balanced_accuracy',\n",
    "#                   n_jobs=-1, verbose=10)\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(clf.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "print()\n",
    "\n",
    "clf_pred  = clf.predict(X_test)\n",
    "print('Plain predictions')\n",
    "print('test  BMAC =', bmac(y_test, clf_pred))\n",
    "clf_train = clf.predict(X_train)\n",
    "print('train BMAC =', bmac(y_train, clf_train))\n",
    "\n",
    "print(\"Detailed classification report:\")\n",
    "print()\n",
    "print(\"The model is trained on the full development set.\")\n",
    "print(\"The scores are computed on the full evaluation set.\")\n",
    "print()\n",
    "print(classification_report(y_test, clf_pred, sample_weight = samwe))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Best parameters set found on development set:\n",
    "\n",
    "{'C': 1, 'gamma': 7e-05}\n",
    "\n",
    "Grid scores on development set:\n",
    "\n",
    "0.698 (+/-0.022) for {'C': 1, 'gamma': 7e-05}\n",
    "0.695 (+/-0.018) for {'C': 1, 'gamma': 8e-05}\n",
    "0.696 (+/-0.021) for {'C': 1, 'gamma': 9e-05}\n",
    "\n",
    "Plain predictions\n",
    "test  BMAC = 0.6932773109243698\n",
    "train BMAC = 0.7787403801501723\n",
    "Detailed classification report:\n",
    "\n",
    "The model is trained on the full development set.\n",
    "The scores are computed on the full evaluation set.\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.69      0.57      0.62      56.0\n",
    "           1       0.69      0.79      0.74 45.33333333333334\n",
    "           2       0.68      0.71      0.70      56.0\n",
    "\n",
    "    accuracy                           0.69 157.33333333333334\n",
    "   macro avg       0.69      0.69      0.69 157.33333333333334\n",
    "weighted avg       0.69      0.69      0.68 157.33333333333334"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights[int(np.array([2,0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
